{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbc664c",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c80077",
   "metadata": {},
   "source": [
    "### 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ac96bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f463a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "#glob로 모든 txt file 읽어오기\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "#읽어온 txt file raw_corpus리스트에 문장 단위로 저장\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e2e6921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 문장 10개 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da3ff8",
   "metadata": {},
   "source": [
    "### 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "045f6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7317c3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정제 데이터 구축하기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue #길이 0\n",
    "    if len(sentence.split()) >= 13: continue  #15개 이하(start,end포함)\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "    \n",
    "corpus[:10] #정제결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241394e",
   "metadata": {},
   "source": [
    "### 3. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b24f42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 3125 ...    0    0    0]\n",
      " [   2   34    7 ...    0    0    0]\n",
      " ...\n",
      " [   2  263  192 ...    0    0    0]\n",
      " [   2  127    5 ...    0    0    0]\n",
      " [   2    7   36 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f31b53f6280>\n"
     ]
    }
   ],
   "source": [
    "#tokenize() 함수로 데이터를 Tensor로 변환\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, #단어장의 크기:12,000 이상 \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # corpus를 Tensor로 변환 \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "307dc0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "[   2   50    5   91  304   62   57    9  974 6332    3    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[  50    5   91  304   62   57    9  974 6332    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#단어사전 구축 인덱스 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n",
    "\n",
    "#생성된 텐서를 소스와 타겟으로 분리해 모델 학습\n",
    "src_input = tensor[:, :-1]   #소스 문장을 생성\n",
    "tgt_input = tensor[:, 1:]   #타겟 문장을 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "29b1e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋 객체 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f771f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#총 데이터의 20% 를 평가 데이터셋\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6875049",
   "metadata": {},
   "source": [
    "### 4. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6fe3f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256  # 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024  #모델에 얼마나 많은 일꾼을 둘 것인가\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bdcea237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
       "array([[[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 2.1986695e-04, -1.1806750e-06, -5.5653433e-04, ...,\n",
       "          1.3243180e-04, -7.2162838e-05,  4.6781183e-04],\n",
       "        [ 2.4793742e-04,  4.9657177e-05, -5.3771725e-04, ...,\n",
       "          1.8614915e-04,  1.0492243e-04,  4.5719004e-04],\n",
       "        ...,\n",
       "        [-8.5921347e-04,  2.0635976e-03, -3.0775103e-03, ...,\n",
       "         -2.0881428e-03, -5.0196066e-03, -2.1982586e-04],\n",
       "        [-8.3106931e-04,  2.0398549e-03, -3.0272522e-03, ...,\n",
       "         -2.1556211e-03, -5.0455779e-03, -1.7801191e-04],\n",
       "        [-8.0410868e-04,  2.0211078e-03, -2.9800639e-03, ...,\n",
       "         -2.2161042e-03, -5.0670360e-03, -1.4026616e-04]],\n",
       "\n",
       "       [[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 1.2725045e-04, -1.5368291e-04, -1.0009131e-04, ...,\n",
       "          1.7742322e-04, -9.8310091e-05,  1.6254981e-04],\n",
       "        [ 1.5196665e-04, -6.8302397e-05, -8.3044644e-05, ...,\n",
       "          4.0944849e-04, -1.3365451e-04,  9.1171685e-05],\n",
       "        ...,\n",
       "        [-8.9989475e-04,  2.2047837e-03, -3.0556363e-03, ...,\n",
       "         -1.8835011e-03, -5.1238257e-03, -2.3361979e-04],\n",
       "        [-8.7004737e-04,  2.1627757e-03, -3.0126180e-03, ...,\n",
       "         -1.9733016e-03, -5.1441868e-03, -2.0557140e-04],\n",
       "        [-8.4064208e-04,  2.1271163e-03, -2.9713211e-03, ...,\n",
       "         -2.0540366e-03, -5.1575270e-03, -1.7682253e-04]],\n",
       "\n",
       "       [[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 3.9606338e-04,  3.9226065e-06, -3.7926823e-04, ...,\n",
       "          4.0878946e-04, -1.2432692e-04,  4.7141023e-04],\n",
       "        [ 6.2188756e-04,  1.2392303e-04, -6.3967542e-04, ...,\n",
       "          4.6058907e-04, -4.6315338e-04,  6.8188802e-04],\n",
       "        ...,\n",
       "        [-8.0505660e-04,  2.1679418e-03, -3.0102809e-03, ...,\n",
       "         -2.0503167e-03, -5.0330162e-03, -2.8370056e-04],\n",
       "        [-7.9016481e-04,  2.1384393e-03, -2.9634412e-03, ...,\n",
       "         -2.1260779e-03, -5.0606090e-03, -2.4822680e-04],\n",
       "        [-7.7412499e-04,  2.1122792e-03, -2.9207638e-03, ...,\n",
       "         -2.1926018e-03, -5.0817924e-03, -2.1308572e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 5.0686427e-05,  8.6271248e-05, -8.7294953e-05, ...,\n",
       "          8.0663078e-05, -2.0110323e-04,  8.8972651e-05],\n",
       "        [ 2.6367098e-04, -5.6410761e-05,  9.3730545e-05, ...,\n",
       "          3.1401732e-04, -2.9386082e-04,  2.2601245e-04],\n",
       "        ...,\n",
       "        [-6.7545928e-04,  2.0237661e-03, -2.7700155e-03, ...,\n",
       "         -2.4183274e-03, -5.1645921e-03, -7.5062650e-05],\n",
       "        [-6.6694233e-04,  2.0136693e-03, -2.7468672e-03, ...,\n",
       "         -2.4504084e-03, -5.1656133e-03, -5.1247891e-05],\n",
       "        [-6.5901590e-04,  2.0056351e-03, -2.7263842e-03, ...,\n",
       "         -2.4779439e-03, -5.1664752e-03, -3.0262141e-05]],\n",
       "\n",
       "       [[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 2.9439933e-04,  9.1840549e-05, -4.3523376e-04, ...,\n",
       "          2.8280370e-04, -2.0615972e-04,  4.9804820e-04],\n",
       "        [ 5.3926290e-04,  2.4246155e-04, -6.6765369e-04, ...,\n",
       "          4.9384631e-04, -1.7904873e-04,  7.4499473e-04],\n",
       "        ...,\n",
       "        [-6.5838685e-04,  2.0084369e-03, -2.8019573e-03, ...,\n",
       "         -2.3278580e-03, -5.1931269e-03, -1.0214883e-04],\n",
       "        [-6.4990739e-04,  2.0021147e-03, -2.7745273e-03, ...,\n",
       "         -2.3707810e-03, -5.1903441e-03, -7.3080431e-05],\n",
       "        [-6.4234261e-04,  1.9970851e-03, -2.7502305e-03, ...,\n",
       "         -2.4081080e-03, -5.1877475e-03, -4.7441343e-05]],\n",
       "\n",
       "       [[ 3.7488193e-05,  3.1478823e-05, -2.1470565e-04, ...,\n",
       "          1.6127748e-04, -4.9667480e-05,  2.3050488e-04],\n",
       "        [ 1.2382687e-04,  2.8341750e-04, -4.1757608e-04, ...,\n",
       "          3.7681736e-04, -1.7241578e-04,  4.8008544e-04],\n",
       "        [-9.9278717e-05,  3.0059234e-04, -3.1119084e-04, ...,\n",
       "          5.6335510e-04, -5.1057088e-04,  3.9302121e-04],\n",
       "        ...,\n",
       "        [-8.0494484e-04,  1.9711677e-03, -2.9708629e-03, ...,\n",
       "         -2.2726585e-03, -5.1078340e-03, -2.5950608e-04],\n",
       "        [-7.8078819e-04,  1.9632920e-03, -2.9244199e-03, ...,\n",
       "         -2.3239423e-03, -5.1186956e-03, -2.1321262e-04],\n",
       "        [-7.5817702e-04,  1.9580715e-03, -2.8824601e-03, ...,\n",
       "         -2.3683866e-03, -5.1273368e-03, -1.7169569e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오기\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4e09233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "538372b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 484s 778ms/step - loss: 1.4346\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 424s 684ms/step - loss: 1.2281\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 235s 379ms/step - loss: 1.1173\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 236s 380ms/step - loss: 1.0222\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 235s 380ms/step - loss: 0.9362\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 236s 381ms/step - loss: 0.8568\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 236s 380ms/step - loss: 0.7832\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 236s 381ms/step - loss: 0.7161\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 236s 381ms/step - loss: 0.6567\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 237s 382ms/step - loss: 0.6060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f31b5058340>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 학습하기\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e62f9",
   "metadata": {},
   "source": [
    "epoch를 10번 실행시켜 loss가 0.6까지 떨어졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8baf893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    while True:\n",
    "        # 입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor) \n",
    "        # 예측된 값 중 가장 높은 확률인 word index를 뽑기\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 예측된 word index를 문장 뒤에 붙임\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d2512dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , baby , so much dready got a job to do <end> '"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0705d4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXPLORATION 04 회고\n",
    "\n",
    "여태까지 진행한 익스들은 프로젝트 전 실습을 하며 이해가 된 상태로 넘어가서, 프로젝트에 바로 응용할 수 있었는데 이번 익스는 연극 언어모델 실습을 하며 내가 이해를 하고 있는건지 처음 접하는거라 원래 이렇게 이해가 안가는건지 싶었다. 그래도 실습을 하며 필기한 내용 토대로 프로젝트를 진행하니 어떤 부분을 수정해야하는지, 어떤 부분이 헷갈렸던건지 이해 할 수 있었다. 무튼 자연어처리는 나에겐 매우 복잡하고 어려운 것이었는데 .. 프로젝트를 통해 또 한 단계 배울 수 있는 새로운 경험이었다.\n",
    "\n",
    "-프로젝트를 진행하며 모호한 점  \n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성> 부분에서 이전에 했던 실습을 참고하여 코드를 짰지만 계속 실패했다. 알고보니 가장 기본적이었던 import를 안해줬던 것.. 방법을 찾는데 매우 오래 걸렸다. 이 과정에서 random_state를 해줘야할지말지 고민하다가 결국 필수가 아니라는 생각에 적용하지 않았는데, random_state를 해야하는지? 하면 무슨 의미가 있는지 모호했다.\n",
    "\n",
    "-프로젝트를 진행하며 아쉬웠던 점  \n",
    "(1) 15개 이하로 만드는 과정에서 모든 데이터를 포괄하고 싶은데 함수를 몰라서 찾아봤다. 현재 적용한 split과 re.findall 두 가지가 가능했지만, re.findall은 반복하다보니 halleluya만 나오는 상황이 발생하여 sentence.split를 사용했다. 왜 한 문장만 반복되는지 궁금하기도 하며 아쉬웠다. 또한, 토큰화를 할 때 15개 이하로 만드는 과정을 포함하는 방법도 시도해보았는데 실패했다. 예상으로는 if를 무조건 사용하면서 else로 다른 걸 걸어줘야 하는 것 같은데.. 아무튼 나는 오류가 나서 아쉬웠다.  \n",
    "(2) generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)라서 처음엔 lyricist->model로 바꿔서 문자 생성에 성공했다. 그래서 모든 model->lyricist로 수정해서 실행했는데, i love까지만 문자 생성을 하고 실패했다. lyricist로 출력하지 못하고 최종적으로는 model로만 출력해 아쉬웠다.  \n",
    "\n",
    "-프로젝트를 진행하며 궁금했던 점  \n",
    "epoch가 처음엔 굉장히 빨리되었는데 계속 실행하다보니 매우 느려졌다. 마지막에는 확인해보니 620까지 올라갔다. 처음 할 땐 5분정도 걸렸는데 마지막엔 1시간 이상을 투자해야 했다. 실행할수록 epoch가 커지는 이유가 궁금했다. 이미 배운건데 기억을 못하는건가?아니면 당연한건가. 검색해봐도 검색식이 잘못되었는지 나오지 않았다. 처음 시도할 땐 epoch 30으로 0.789라는 결과를 만들었는데, 최종적으로는 epoch를 10으로 실행시켜도 오랜 시간이 걸렸기 때문에 10으로만 했다.\n",
    "\n",
    "    \n",
    "2022-01-20\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
